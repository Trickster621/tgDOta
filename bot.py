import logging
import os
from urllib.parse import urljoin

import cloudscraper
from bs4 import BeautifulSoup
from telegram import Update
from telegram.ext import Application, CommandHandler, MessageHandler, filters, ContextTypes

# === –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ===
BOT_TOKEN = os.getenv("BOT_TOKEN", "–í–ê–®_–¢–û–ö–ï–ù")
BASE_URL = "https://dota1x6.com"

# –õ–æ–≥–≥–µ—Ä
logging.basicConfig(
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    level=logging.INFO,
)
logger = logging.getLogger(__name__)

# Scraper –¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare
scraper = cloudscraper.create_scraper()


# === –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π ===
def fetch_updates_list_first_item():
    """–ü–∞—Ä—Å–∏—Ç HTML /updates –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å {title, url} –¥–ª—è —Å–∞–º–æ–≥–æ —Å–≤–µ–∂–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è."""
    try:
        r = scraper.get(f"{BASE_URL}/updates", timeout=10)
        if r.status_code != 200:
            logger.warning("Updates page returned %s", r.status_code)
            return None
        soup = BeautifulSoup(r.text, "html.parser")

        # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∫–∞—Ä—Ç–æ—á–∫—É
        first_card = soup.find("div", class_="card")
        if not first_card:
            logger.warning("no .card found on updates page")
            return None

        link_tag = first_card.find("a", href=True)
        if not link_tag:
            logger.warning("no <a> in first card")
            return None

        title = link_tag.get_text(strip=True)
        link = urljoin(BASE_URL, link_tag["href"])

        return {"title": title, "url": link}
    except Exception as e:
        logger.warning("Cannot parse updates list: %s", e)
        return None


def fetch_update_detail(link):
    """–ó–∞–±–∏—Ä–∞–µ—Ç —Ç–µ–∫—Å—Ç —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è"""
    try:
        r = scraper.get(link, timeout=10)
        if r.status_code != 200:
            logger.warning("Update detail returned %s", r.status_code)
            return None

        soup = BeautifulSoup(r.text, "html.parser")
        content_div = soup.find("div", class_="news-detail")
        if not content_div:
            logger.warning("no news-detail content")
            return None

        # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç
        paragraphs = content_div.find_all(["p", "li"])
        text = "\n".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))
        return text
    except Exception as e:
        logger.warning("Cannot parse update detail: %s", e)
        return None


# === –ö–æ–º–∞–Ω–¥—ã –±–æ—Ç–∞ ===
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text("–ü—Ä–∏–≤–µ—Ç! –Ø –±–æ—Ç. –ò—Å–ø–æ–ª—å–∑—É–π /last_update —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ.")


async def last_update(update: Update, context: ContextTypes.DEFAULT_TYPE):
    item = fetch_updates_list_first_item()
    if not item:
        await update.message.reply_text("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π")
        return

    detail = fetch_update_detail(item["url"])
    if not detail:
        await update.message.reply_text(f"–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ: {item['title']}\n{item['url']}")
    else:
        text = f"üìå {item['title']}\n\n{detail[:3500]}"  # –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –¥–ª–∏–Ω—É
        await update.message.reply_text(text)


# === Main ===
def main():
    logger.info("Bot started")
    app = Application.builder().token(BOT_TOKEN).build()

    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("last_update", last_update))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, start))

    app.run_polling()


if __name__ == "__main__":
    main()
